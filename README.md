# Artificial-Intelligence-CSE643---M2024
Artificial Intelligence (CSE643) - M2024- Repo for practice concepts with PYQs Sol with Assignments


| Component | Original Design | Revised Design | Why |
| :--- | :--- | :--- | :--- |
| **LSTM Block** | Separate LSTM cells drawn for each lagged input (e.g. one LSTM for $x_{t-p}$, another for $x_{t-p+1}$, etc.), feeding into one another in series. | A single (stacked) LSTM that consumes the entire sequence $\{x_{t-p},\dots,x_t\}$ at once. In practice: two LSTM layers (e.g. 128→64), taking the last hidden state. | Running one LSTM (or a small stack) over the full $(p+1)$-step window is far more parameter-efficient, easier to implement in code, and captures temporal dependencies in one cohesive block. |
| **Normalization / BatchNorm / Dropout** | A single “BatchNorm → Normalization → Dropout” block was drawn between the final LSTM and Dense, with no clear indication which tensor it normalized. | Split into: BatchNorm1d applied to the last hidden of LSTM2 (size 64). Dropout immediately after each LSTM layer and after each Dense. Offline feature-scaling done once before training. | Placing BatchNorm directly after the LSTM hidden state (not as a standalone “mystery” block) stabilizes gradients. Distributing Dropout across layers (instead of one giant block) yields better regularization and clarity. |
| **FinBERT Encoders** | Twelve separate encoder blocks drawn in parallel, as if each headline went through 12 parallel Transformers. | One FinBERT stack (12 encoder layers in series). Each headline’s tokenized input flows through all 12 layers serially; we extract the final [CLS] embedding. | In reality, FinBERT is a 12-layer stack, not 12 parallel models. Drawing them in series clarifies the true computation graph, avoids parameter duplication, and matches the HuggingFace/TensorFlow implementations. |
| **Multiple Headlines → Pooling** | “News Selection” arrow was ambiguous (FinBERT → Sentiment Scores → then back into separate encoder blocks). No explicit pooling shown. | After projecting each headline’s [CLS] (768→64) via a small Dense + ReLU + Dropout, perform average pooling (or attention pooling) over all $N$ headlines to produce one 64-dim vector $v_{\text{text}}$. | If you have multiple headlines per day, you must aggregate them into a single “daily news vector.” Showing explicit average (or attention) pooling clarifies how to handle days with 0, 1, or many headlines. |
| **Fusion Head** | Simple “Concatenate → Dense → Final Prediction” block (just one Dense immediately after concatenation). | Concatenate $[v_{\text{price}}‖v_{\text{text}}]$ (128 dims), then: Dense 128 → ReLU → Dropout, Dense 64 → ReLU → Dropout, Final out-layer (1 node for regression or binary classification). | Two successive Dense layers (with activations and dropout) give the network more capacity to learn complex cross-modal interactions. A single Dense could underfit multimodal feature combinations. |
| **FinBERT Fine-tuning** | No indication of which layers to freeze or fine-tune in FinBERT. | Freeze layers 1–10 of FinBERT, fine-tune layers 11–12 (and the small Dense_text projection). | FinBERT has ~110 M parameters. Freezing most layers prevents overfitting when data is limited and speeds up training. Only fine-tuning the top 1–2 layers yields good performance with fewer trainable parameters. |
| **Final Output & Loss** | No distinction between regression vs. classification in architecture drawing. | Explicitly build a final Linear(in=64, out=1) that—for regression returns a real value (no activation), or for binary classification you apply sigmoid + BCEWithLogitsLoss. | Clarifying whether you predict a real-valued next-day return or a binary up/down class changes the final activation and loss. Making that explicit avoids confusion at implementation time. |
